{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install rank_bm25"
      ],
      "metadata": {
        "id": "zkRtjaFfR781"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "xllsH8P5OXQR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "from rank_bm25 import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding words to stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jMv0SOmXnJG",
        "outputId": "e7457d1a-5850-483d-9130-b4330b1923de"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%markdown\n",
        "# Load and process CISI dataset "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "5ezrb6YQO_kc",
        "outputId": "1f7ab8fa-60e4-4098-d2fb-1b979ba6f869"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Load and process CISI dataset \n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder_path = '/content/drive/MyDrive/doutorado/P_IA368DD_2023S/t1/cisi/'\n",
        "\n",
        "# Processing all documents \n",
        "doc_set = {}\n",
        "doc_id = \"\"\n",
        "doc_text = \"\"\n",
        "with open(dataset_folder_path+'CISI.ALL') as f:\n",
        "    lines = \"\"\n",
        "    for l in f.readlines():\n",
        "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
        "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
        "doc_count = 0\n",
        "for l in lines:\n",
        "    if l.startswith(\".I\"):\n",
        "        doc_id = int(l.split(\" \")[1].strip())-1\n",
        "    elif l.startswith(\".X\"):\n",
        "        doc_set[doc_id] = doc_text.lstrip(\" \")\n",
        "        doc_id = \"\"\n",
        "        doc_text = \"\"\n",
        "    else:\n",
        "        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.    \n",
        "\n",
        "        \n",
        "# Processing queries\n",
        "with open(dataset_folder_path+'CISI.QRY') as f:\n",
        "    lines = \"\"\n",
        "    for l in f.readlines():\n",
        "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
        "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
        "    \n",
        "qry_set = {}\n",
        "qry_id = \"\"\n",
        "for l in lines:\n",
        "    if l.startswith(\".I\"):\n",
        "        qry_id = int(l.split(\" \")[1].strip()) -1\n",
        "    elif l.startswith(\".W\"):\n",
        "        qry_set[qry_id] = l.strip()[3:]\n",
        "        qry_id = \"\"\n",
        "\n",
        "# Processing Relevance assesments\n",
        "rel_set = {}\n",
        "with open(dataset_folder_path+'CISI.REL') as f:\n",
        "    for l in f.readlines():\n",
        "        qry_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]) -1\n",
        "        doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])-1\n",
        "        if qry_id in rel_set:\n",
        "            rel_set[qry_id].append(doc_id)\n",
        "        else:\n",
        "            rel_set[qry_id] = []\n",
        "            rel_set[qry_id].append(doc_id)"
      ],
      "metadata": {
        "id": "rv6eipKhPs1D"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(13)\n",
        "idx = random.sample(rel_set.keys(),1)[0]\n",
        "\n",
        "print('Query ID %s ==>' % idx, qry_set[idx])\n",
        "rel_docs = rel_set[idx]\n",
        "print('Documents relevants to Query ID %s' % idx, rel_docs)\n",
        "sample_document_idx = random.sample(rel_docs,1)[0]\n",
        "print('Document ID %s ==>' % sample_document_idx, doc_set[sample_document_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceVY-U4qQhmR",
        "outputId": "2d35f359-8007-4073-b4bc-35d17b9f5d6e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query ID 33 ==> Methods of coding used in computerized index systems.\n",
            "Documents relevants to Query ID 33 [33, 43, 52, 56, 67, 79, 116, 122, 158, 319, 376, 433, 488, 492, 521, 529, 599, 667, 668, 669, 670, 672, 676, 677, 678, 680, 686, 688, 693, 696, 698, 699, 703, 705, 707, 708, 829, 1143]\n",
            "Document ID 668 ==> Rapid Structure Searches via Permuted Chemical Line-Notations Sorter, P.F. Granito, C.E. Gilmer, J.C. Gelberg, A. Metcalf, E.A. The Wiswesser chemical line-notation is an unique and unambiguous method of representing chemical structures by a linear series of letters, numbers, ampersands, and hyphens.  These symbols are meaningful to chemists familiar with the notation and can be processed by automatic data processing (ADP) equipment. The uniqueness of the line-notation permits the use of alphanumerically arranged lists of notations for dictionary-type searches.  This ordered arrangement permits the rapid location of a specific compound or a specific class of ring compounds other than benzenoid. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing special characters:\n",
        "def special_chars_removal(lst):\n",
        "    lst1=list()\n",
        "    for element in lst:\n",
        "        str=\"\"\n",
        "        str = re.sub(r'[^a-zA-Z\\s]', '', element)\n",
        "        lst1.append(str)\n",
        "    return lst1\n",
        "\n",
        "#Normalization words\n",
        "def stopwprds_removal(lst):\n",
        "    lst1=list()\n",
        "    for string in lst:\n",
        "        text_tokens = word_tokenize(string)\n",
        "        tokens_without_sw = [word for word in text_tokens if word.casefold() not in STOPWORDS]\n",
        "        \n",
        "        # Lemmatize the remaining words (reduce to their base form)\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_without_sw]\n",
        "\n",
        "        # Join the lemmatized words back into a string\n",
        "        normalized_text = \" \".join(lemmatized_words)\n",
        "\n",
        "        lst1.append(normalized_text)#str_t)\n",
        " \n",
        "    return lst1"
      ],
      "metadata": {
        "id": "GarhKPDATyds"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = qry_set[idx] #get query\n",
        "rel_docs = rel_set[idx] #relevant documents\n",
        "\n",
        "corpus = list(doc_set.values())\n",
        "\n",
        "# preprocess the documents\n",
        "##removing special characters\n",
        "corpus_without_sc = special_chars_removal(corpus)\n",
        "## normalize texts\n",
        "corpus_normalized = stopwprds_removal(corpus_without_sc)\n",
        "\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus_normalized]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n"
      ],
      "metadata": {
        "id": "_6RvueiQRsOM"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%markdown\n",
        "# Top Five associated data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "unEZWtp6Xyew",
        "outputId": "e8b1020f-e489-4c39-ec72-16016ba199c3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Top Five associated data\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process query\n",
        "tokenized_query = query.split(\" \")\n",
        "print('Query ==> ', query, '\\nRelevant documents IDs: ==> ', rel_docs)\n",
        "\n",
        "doc_scores = bm25.get_scores(tokenized_query)\n",
        "print(doc_scores, len(doc_scores), len(doc_scores))\n",
        "\n",
        "docs = bm25.get_top_n(tokenized_query, corpus_normalized, n=5)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MShnDM6To8J",
        "outputId": "9e69fdca-15de-4afe-d59b-d989e4685009"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query ==>  Methods of coding used in computerized index systems. \n",
            "Relevant documents IDs: ==>  [33, 43, 52, 56, 67, 79, 116, 122, 158, 319, 376, 433, 488, 492, 521, 529, 599, 667, 668, 669, 670, 672, 676, 677, 678, 680, 686, 688, 693, 696, 698, 699, 703, 705, 707, 708, 829, 1143]\n",
            "[0. 0. 0. ... 0. 0. 0.] 1460 1460\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Truncated Search Key Title Index Long Philip L Kilgour Frederick G experiment showing search key derived title sufficiently specific efficient computerized interactive index file MARC II record',\n",
              " 'Notation Coding Organic Compounds Geivandov E notation coding organic structure developed provides simple rational rule coding common cyclic fragment conventional unit coding regular structure benzol ring skeleton regular condensed coded sequence odd integer set rule coding regular system component universal notation organic compound code offered author designed cover important broad class compound conjugate bond framework specialized computerbased information retrieval capacity input internal machine language',\n",
              " 'Method Construction MinimumRedundancy Codes Huffman David optimum method coding ensemble message consisting finite number member developed minimumredundancy code constructed way average number coding digit message minimized',\n",
              " 'Thesaurofacet Multipurpose Retrieval Language Tool Aitchison Jean description given English Electric Thesaurofacet faceted classification thesaurus covering engineering related scientific technical management subject novel feature integration classification schedule thesaurus term appears thesaurus schedule schedule term displayed appropriate facet hierarchy thesaurus supplement information indicating alternative hierarchy relationship cut classified arrangement thesaurus control word form synonym act alphabetical index class number resulting tool multipurpose easily applicable shelf arrangement conventional classified card catalogue coordinate indexing computerized retrieval system reason given modifying certain traditional facet technique including choice traditional discipline main class lack builtin preferred order use certain instance enumeration synthesis express multiterm concept Methods application Thesaurofacet precoordinate postcoordinate system discussed brief account given technique employed compilation',\n",
              " 'Catalog Records retrieved Pesonal Author Derived Search Keys Landgraf Alan L Kilgour frederick G investigation show search key derived personal author name posse sufficient degree distinctness employed efficient computerized interactive index file MARC II catalog record having personal author entry']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yISStMv2azPW"
      },
      "execution_count": 73,
      "outputs": []
    }
  ]
}