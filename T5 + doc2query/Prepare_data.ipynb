{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "66XbvOjF9Hwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/doutorado/P_IA368DD_2023S/aula5')"
      ],
      "metadata": {
        "id": "5lHmYpy65lmH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "j-XyXNIA9I8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%markdown\n",
        "# Download and split MSMARCO Triples in Train-test datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "CaCCzQWt5hjz",
        "outputId": "de770a3b-becb-489f-c94d-c0dfc6d208b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Download and split MSMARCO Triples in Train-test datasets\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65jFI4Wy5WDo"
      },
      "outputs": [],
      "source": [
        "#download datatset\n",
        "!wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('data', exist_ok=True)\n",
        "# Get the last 1000 for validation\n",
        "!tail -n 1000 msmarco_triples.train.tiny.tsv > data/validation.tsv\n",
        "\n",
        "# Get the first 10000 for training\n",
        "!head -n 10000 msmarco_triples.train.tiny.tsv > data/train.tsv\n",
        "     "
      ],
      "metadata": {
        "id": "jnkhRP6a5b2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%markdown\n",
        "#TREC-COVID 2020"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "N0_0lmw-6Jyw",
        "outputId": "7344427b-208e-4364-c1c8-af26f387f641"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#TREC-COVID 2020\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('TREC_COVID_2020', exist_ok=True)\n",
        "# Corpus with all passages\n",
        "!wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz\n",
        "!gzip -cd corpus.jsonl.gz > TREC_COVID_2020/corpus.jsonl\n",
        "\n",
        "\n",
        "# Topics with all queries\n",
        "!wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/queries.jsonl.gz\n",
        "!gzip -dc queries.jsonl.gz > TREC_COVID_2020/queries.jsonl\n",
        "\n",
        "\n",
        "# Qrels with all relevances\n",
        "!wget https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv"
      ],
      "metadata": {
        "id": "IGmWSBda6KMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"TREC_COVID_2020/qrels.tsv\", \"w\") as fout:\n",
        "    with open(\"TREC_COVID_2020/test.tsv\", \"r\") as fin:\n",
        "        for idx, line in enumerate(fin):\n",
        "            if idx != 0:\n",
        "                qid, doc_id, relevance = line.strip().split(\"\\t\")\n",
        "                fout.write(f\"{qid}\\t0\\t{doc_id}\\t{relevance}\\n\")"
      ],
      "metadata": {
        "id": "nTWsQ5ND6VaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('TREC_COVID_2020/corpus_original', exist_ok=True)\n",
        "\n",
        "corpus = {}\n",
        "with open(\"TREC_COVID_2020/corpus_original/corpus_original.jsonl\", \"w\") as fout:\n",
        "    with open(\"TREC_COVID_2020/corpus.jsonl\", \"r\") as fin:\n",
        "        for line in fin:\n",
        "            doc = json.loads(line)\n",
        "            corpus[doc[\"_id\"]] = f\"{doc['title']}. {doc['text']}\"\n",
        "            doc_dict = {\n",
        "                \"id\": doc[\"_id\"],\n",
        "                \"contents\": f\"{doc['title']}. {doc['text']}\"\n",
        "            }\n",
        "            fout.write(json.dumps(doc_dict) + \"\\n\")"
      ],
      "metadata": {
        "id": "484tdfXw6Vs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%markdown\n",
        "# Start expanding corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "IWbzxAHA6V26",
        "outputId": "89ea6aaf-4419-4eaf-9c62-a5edb60e054e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Start expanding corpus\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "d5HBDVp47y2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"results/final_model\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"results/final_model\").to(device)"
      ],
      "metadata": {
        "id": "UCtUwKdN7tWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(corpus, batch_size=256):\n",
        "    ids = list(corpus.keys())\n",
        "    texts = list(corpus.values())\n",
        "    for i in tqdm(range(0, len(ids), batch_size), desc=\"Expanding documents\"):\n",
        "        yield ids[i:i+batch_size], texts[i:i+batch_size]"
      ],
      "metadata": {
        "id": "q-GaRh3J9BEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('TREC_COVID_2020/expanded_corpus', exist_ok=True)\n",
        "\n",
        "with open(\"TREC_COVID_2020/expanded_corpus/expanded_corpus.jsonl\", \"w\") as fout:\n",
        "    for ids, batch in get_batch(corpus, 256):\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "        \n",
        "        output_sequences = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            do_sample=False,\n",
        "            max_new_tokens=32\n",
        "        )\n",
        "        new_queries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "        for id, text, query in zip(ids, batch, new_queries):\n",
        "            fout.write(json.dumps({\n",
        "                \"id\": id,\n",
        "                \"contents\": f\"{query}. {text}\"\n",
        "            }) + \"\\n\")\n",
        "     "
      ],
      "metadata": {
        "id": "rPUQwsJO7xit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eto25QVu78qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6MfgFIdJ788X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}